# -*- coding: utf-8 -*-
"""exp_lr_efficient_net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZgzhyzKUmKDDUMvNIE3ospjTND2vn8HF
"""

from google.colab import drive 
drive.mount('/content/MyDrive')
import sys
sys.path.append('/content/MyDrive/MyDrive/Model_files/MiDaS')

!pip install utils

import sys

sys.path.append("..")

import torch
import torch.nn as nn
import os
import torch
import numpy as np

class conv1x1(nn.Module):
    def __init__(self, channel_in, channel_out):
        super(conv1x1, self).__init__()
        self.conv = nn.ConvTranspose2d(
            channel_in, channel_out, kernel_size=3, stride=1, padding=1
        )

    def forward(self, x):
        return self.conv(x)

class conv1x1_Model2(nn.Module):
    def __init__(self, channel_in, channel_out):
        super(conv1x1_Model2, self).__init__()
        self.conv = nn.ConvTranspose2d(
            channel_in, channel_out, kernel_size=5, stride=2, padding=2, output_padding=1,
        )

    def forward(self, x):
        return self.conv(x)


class conv5x5(nn.Module):
    def __init__(self, channel_in, channel_out):
        super(conv5x5, self).__init__()
        self.conv = nn.Conv2d(
            channel_in, channel_out, kernel_size=5, stride=1, padding=2
        )
        self.activation = nn.ReLU()
        self.bnorm = nn.BatchNorm2d(channel_out)

    def forward(self, x):
        return self.bnorm(self.activation(self.conv(x)))

class conv3x3(nn.Module):
  def __init__(self,channel_in,channel_out):
    super(conv3x3,self).__init__()
    self.conv = nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1)
    self.bnorm = nn.BatchNorm2d(channel_out)
    self.activation = nn.ReLU()
  def forward(self, x):
    return self.bnorm(self.activation(self.conv(x)))


class conv5x5_leakR(nn.Module):
  def __init__(self,channel_in,channel_out):
    super(conv5x5_leakR,self).__init__()
    self.conv = nn.Conv2d(channel_in, channel_out, kernel_size=5, stride=1, padding=2)
    self.bnorm = nn.BatchNorm2d(channel_out)
    self.activation = nn.LeakyReLU()
  def forward(self, x):
    return self.bnorm(self.activation(self.conv(x)))

class conv3x3_leakR(nn.Module):
  def __init__(self,channel_in,channel_out):
    super(conv3x3_leakR,self).__init__()
    self.conv = nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1)
    self.bnorm = nn.BatchNorm2d(channel_out)
    self.activation = nn.LeakyReLU()
  def forward(self, x):
    return self.bnorm(self.activation(self.conv(x)))


class upconv3x3(nn.Module):
    def __init__(self, channel_in, channel_out):
        super(upconv3x3, self).__init__()
        self.conv = nn.ConvTranspose2d(
            channel_in, channel_out, kernel_size=4, stride=2, padding=1
        )
        self.activation = nn.LeakyReLU()
        self.bnorm = nn.BatchNorm2d(channel_out)

    def forward(self, x):
        return self.bnorm(self.activation(self.conv(x)))

# Commented out IPython magic to ensure Python compatibility.
# %cd MyDrive/MyDrive/Model_files/MiDaS

import os
import glob
import torch
import utils
import cv2
import argparse

from torchvision.transforms import Compose
# from midas.midas_net import MidasNet
from midas.midas_net_custom import MidasNet_small
from midas.transforms import Resize, NormalizeImage, PrepareForNet

import re
import numpy as np
import sys


def readPFM(file):
    file = open(file, 'rb')

    color = None
    width = None
    height = None
    scale = None
    endian = None

    header = file.readline().rstrip()
    if header == b'PF':
        color = True
    elif header == b'Pf':
        color = False
    else:
        raise Exception('Not a PFM file.')

    dim_match = re.match(r'^(\d+)\s(\d+)\s$', file.readline().decode('utf-8'))
    if dim_match:
        width, height = map(int, dim_match.groups())
    else:
        raise Exception('Malformed PFM header.')

    scale = float(file.readline().rstrip())
    if scale < 0:  # little-endian
        endian = '<'
        scale = -scale
    else:
        endian = '>'  # big-endian

    data = np.fromfile(file, endian + 'f')
    shape = (height, width, 3) if color else (height, width)

    data = np.reshape(data, shape)
    data = np.flipud(data)
    file.close()
    return data, scale

IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG',
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',
]

from PIL import Image

def is_image_file(filename):
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)


def default_loader(path):
    return Image.open(path).convert('RGB')


def disparity_loader(path):
    return readPFM(path)

def PFM_loader(path):
    return readPFM(path)

import random
import os
folder = 'MyDrive/MyDrive/Model_files/MiDaS/midas'
def dataloader(filepath):
    all_left_img = []
    all_right_img = []
    all_left_disp = []
    all_left_depth_clue = []
    for filename in os.listdir(filepath):
      all_left_img.append(filepath + str(filename) + '/left.png')
      all_right_img.append(filepath + str(filename) + '/right.png')
      all_left_disp.append(filepath + str(filename) + '/gt_left.pfm')
      all_left_depth_clue.append(filepath + str(filename) + '/DistgDisp.pfm')
    return all_left_img, all_right_img, all_left_disp, all_left_depth_clue
#dataloader('/content/MyDrive/MyDrive/Stereo_dataset_with_central_view/Train/')

import torch.utils.data as data
import numpy as np
import torchvision
class myImageFloder(data.Dataset):
    def __init__(self, left, right, left_disparity, depth_clue, training, loader=default_loader):#, dploader=disparity_loader):

        self.left = left
        self.right = right
        self.disp_L = left_disparity
        self.depth_clue = depth_clue
        self.loader = loader
        self.dploader = disparity_loader
        self.dcloader = PFM_loader
        self.training = training

    def __getitem__(self, index):
        left = self.left[index]
        right = self.right[index]
        disp_L = self.disp_L[index]
        depth_clue = self.depth_clue[index]

        datagt, scaleL = self.dploader(disp_L)
        datagt = np.ascontiguousarray(datagt, dtype=np.float32)
        
        datadc, scaleL = self.dcloader(depth_clue)
        datadc = np.ascontiguousarray(datadc, dtype=np.float32)
        th, tw = 256, 512

        left_img = np.array(self.loader(left))
        right_img = np.array(self.loader(right))
        
        left_img = cv2.resize(left_img,(tw,th),interpolation = cv2.INTER_CUBIC)
        right_img = cv2.resize(right_img,(tw,th),interpolation = cv2.INTER_CUBIC)
        left_img = np.rint(255*((left_img-left_img.min())/(left_img.max()-left_img.min())))
        right_img = np.rint(255*((right_img-right_img.min())/(right_img.max()-right_img.min())))
        
        left_img = np.transpose(left_img,(2,0,1)).astype(np.float32)
        right_img = np.transpose(right_img,(2,0,1)).astype(np.float32)
        left_img = (torch.from_numpy(np.array(left_img)))
        right_img = (torch.from_numpy(np.array(right_img)))

        datagt = cv2.resize(datagt,(tw,th),interpolation = cv2.INTER_CUBIC)
        datagt = np.rint(255*((datagt-datagt.min())/(datagt.max()-datagt.min())))
        datagt = (torch.from_numpy(np.array(datagt)))
        #datagt = datagt.unsqueeze(0)

        datadc = cv2.resize(datadc,(tw,th),interpolation = cv2.INTER_CUBIC)
        datadc = np.rint(255*((datadc-datadc.min())/(datadc.max()-datadc.min())))
        datadc = (torch.from_numpy(np.array(datadc)))
        #datadc = datadc.unsqueeze(0)
        
      
        return left_img, right_img, datagt, datadc

    def __len__(self):
        return len(self.left)

import torch,os
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
datapath = '/content/MyDrive/MyDrive/Model_files/Stereo_dataset_with_central_view/Train/'
train_left_img, train_right_img, train_left_disp, train_left_depth_clue = dataloader(datapath)
total = int(len(train_left_disp))
trainCount = int(0.9*total)

test_left_img, test_right_img, test_left_disp, test_left_depth_clue= train_left_img[trainCount:], train_right_img[trainCount:], train_left_disp[trainCount:], train_left_depth_clue[trainCount:]

train_left_img, train_right_img, train_left_disp, train_left_depth_clue = train_left_img[:trainCount], train_right_img[:trainCount], train_left_disp[:trainCount], train_left_depth_clue[:trainCount]
print('train: ',len(train_left_img), 'val: ',len(test_left_img))

TrainImgLoader = torch.utils.data.DataLoader(
    myImageFloder(train_left_img, train_right_img, train_left_disp, train_left_depth_clue, True),
    batch_size=4, shuffle=True, num_workers=2, drop_last=False)

TestImgLoader = torch.utils.data.DataLoader(
    myImageFloder(test_left_img, test_right_img, test_left_disp, test_left_depth_clue, False),
    batch_size=1, shuffle=False, num_workers=2, drop_last=False)

from tqdm.notebook import tqdm

"""# Model 10 efficient_attn"""

import torch
import math
import torch.nn as nn
import torch.nn.functional as F

class base_layer_1(nn.Module):
  def __init__(self, in_channel,out_channel):
    super(base_layer_1, self).__init__()
    self.maxpool = nn.MaxPool2d(2)
    self.convntom = conv3x3_leakR(in_channel, out_channel)
    self.convmtom = conv3x3_leakR(out_channel, out_channel)

  def forward(self,x):
    x = self.convntom(x)
    x1 = self.convmtom(x)
    x = self.maxpool(x1)
    return x,x1

class EfficientAttention(nn.Module):
    
    def __init__(self, in_channels, key_channels, head_count, value_channels):
        super().__init__()
        self.in_channels = in_channels
        self.key_channels = key_channels
        self.head_count = head_count
        self.value_channels = value_channels

        self.keys = nn.Conv2d(in_channels, key_channels, 1)
        self.queries = nn.Conv2d(in_channels, key_channels, 1)
        self.values = nn.Conv2d(in_channels, value_channels, 1)
        self.reprojection = nn.Conv2d(value_channels, in_channels, 1)

    def forward(self, input_):
        n, _, h, w = input_.size()
        keys = self.keys(input_).reshape((n, self.key_channels, h * w))
        queries = self.queries(input_).reshape(n, self.key_channels, h * w)
        values = self.values(input_).reshape((n, self.value_channels, h * w))
        head_key_channels = self.key_channels // self.head_count
        head_value_channels = self.value_channels // self.head_count
        
        attended_values = []
        for i in range(self.head_count):
            key = F.softmax(keys[
                :,
                i * head_key_channels: (i + 1) * head_key_channels,
                :
            ], dim=2)
            query = F.softmax(queries[
                :,
                i * head_key_channels: (i + 1) * head_key_channels,
                :
            ], dim=1)
            value = values[
                :,
                i * head_value_channels: (i + 1) * head_value_channels,
                :
            ]
            context = key @ value.transpose(1, 2)
            attended_value = (
                context.transpose(1, 2) @ query
            ).reshape(n, head_value_channels, h, w)
            attended_values.append(attended_value)

        aggregated_values = torch.cat(attended_values, dim=1)
        reprojected_value = self.reprojection(aggregated_values)
        attention = reprojected_value + input_

        return attention

import torch
import torch.nn as nn
import torch.nn.functional as F



class _2TUnet(nn.Module):
    def __init__(self):
        super(_2TUnet, self).__init__()

        c0, c1, c2, c3, c4, c5 = [64, 128, 256, 512, 1024, 2048]
        self.maxpool = nn.MaxPool2d(2)

        self.layer_1_4to64 = base_layer_1(4,c0)
        self.layer_1_3to64 = base_layer_1(3,c0) 
        self.layer_2_64to128 = base_layer_1(c0,c1)

        self.layer_3_128to256 = base_layer_1(c1,c2)
        self.layer_4_256to512 = base_layer_1(c2,c3)
        self.layer_5_512to1024 = base_layer_1(c3,c4)

        self.conv1024to2048 = conv3x3_leakR(c4,c5)
        
        self.conv64to64 = conv3x3_leakR(c0,c0)
        self.conv128to128 = conv3x3_leakR(c1, c1)
        self.conv256to256 = conv3x3_leakR(c2, c2)
        self.conv512to512 = conv3x3_leakR(c3, c3)
        self.conv1024to1024 = conv3x3_leakR(c4, c4)
        self.conv2048to2048 = conv3x3_leakR(c5, c5)
        
        self.conv2048to2048 = conv3x3_leakR(c5, c5)

        self.conv128to64 = conv3x3_leakR(c1, c0)
        self.conv256to128 = conv3x3_leakR(c2, c1)
        self.conv512to256 = conv3x3_leakR(c3, c2)
        self.conv1024to512 = conv3x3_leakR(c4, c3)
        self.conv2048to1024 = conv3x3_leakR(c5,c4)

        self.upconv2048to1024 = upconv3x3(c5,c4)
        self.upconv1024to512 = upconv3x3(c4, c3)
        self.upconv512to256 = upconv3x3(c3, c2)
        self.upconv256to128 = upconv3x3(c2, c1)
        self.upconv128to64 = upconv3x3(c1, c0)

        self.conv64to1 = conv1x1(c0, 1)

        #Attention maps
        # the number of keys and values must be divisible by the number of heads
        self.effatn64 = EfficientAttention(c0,64,8,64)
        self.effatn128 = EfficientAttention(c1,64,8,64)
        self.effatn256 = EfficientAttention(c2,64,8,64)
        self.effatn512 = EfficientAttention(c3,64,8,64)
        self.effatn1024 = EfficientAttention(c4,64,8,64)
        self.effatn2048 = EfficientAttention(c5,64,8,64)

        # self.MHSA64 = MultiHeadSelfAttention(64)
        # self.MHSA128 = MultiHeadSelfAttention(128)
        # self.MHSA256 = MultiHeadSelfAttention(256)
        # self.MHSA512 = MultiHeadSelfAttention(512)
        # self.MHSA = MultiHeadSelfAttention(512)
        
    def forward(self, x, y, depth_clue):
        #print("x " + str(x.shape))
        #print("y " + str(y.shape))
        #print("depth_clue " + str(depth_clue.shape))
        x = torch.cat([x, depth_clue],dim=1)
        x,x1 = self.layer_1_4to64(x)
        y,y1 = self.layer_1_3to64(y)
        
        x,x2 = self.layer_2_64to128(x)
        y,y2 = self.layer_2_64to128(y)

        x,x3 = self.layer_3_128to256(x)
        y,y3 = self.layer_3_128to256(y)

        x,x4 = self.layer_4_256to512(x)
        y,y4 = self.layer_4_256to512(y)

        x,x5 = self.layer_5_512to1024(x)
        y,y5 = self.layer_5_512to1024(y)
        
        x = self.conv1024to2048(x)
        x = self.conv2048to2048(x)
        #print("self.conv1024to1024(x) " +str(x.shape))
        x = self.upconv2048to1024(x)
        a5 = self.effatn2048(torch.concat([x5,y5],dim=1))
        a5 = self.conv2048to1024(a5)
        x = self.conv2048to1024(torch.cat([x,a5],dim=1))
        x = self.conv1024to1024(x)

        x = self.upconv1024to512(x)
        #print("self.upconv1024to512(x) " +str(x.shape))
        a4 = self.effatn1024(torch.concat([x4,y4],dim=1))
        a4 = self.conv1024to512(a4)
        x = self.conv1024to512(torch.cat([x, a4], dim=1))
        #print("self.conv1024to512(x) " +str(x.shape))

        x = self.conv512to512(x)
        x = self.upconv512to256(x)
        #print("self.upconv512to256(x) " +str(x.shape))
        a3 = self.effatn512(torch.concat([x3,y3],dim=1))
        a3 = self.conv512to256(a3) 
        x = self.conv512to256(torch.cat([x, a3], dim=1))
        #print("self.conv512to256(x) " +str(x.shape))
        x = self.conv256to256(x)
        x = self.upconv256to128(x)
        #print("self.upconv256to128(x) " +str(x.shape))
        a2 = self.effatn256(torch.concat([x2,y2],dim=1))
        a2 = self.conv256to128(a2) 
        x = self.conv256to128(torch.cat([x, a2], dim=1))
        #print("self.conv256to128(x) " +str(x.shape))
        x = self.conv128to128(x)
        x = self.upconv128to64(x)
        #print("self.upconv128to64(x) " +str(x.shape))
        a1 = self.effatn128(torch.concat([x1,y1],dim=1))
        a1 = self.conv128to64(a1) 
        x = self.conv128to64(torch.cat([x, a1], dim=1))
        #print("self.conv128to64(x) " +str(x.shape))
        x = self.conv64to64(x)
        #print("self.conv64to64(x) " +str(x.shape))
        x = self.conv64to1(x)
        #print("x output = " +str(x.shape))
        return x

import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import torch
from skimage.metrics import structural_similarity as ssim
import math
import torch

initial_learning_rate= 0.25
td_lr = initial_learning_rate
def time_decay(epoch):
  lr_decay = 55e-9 #15e-7
  global td_lr
  td_lr *= (math.e)**(-1*lr_decay*epoch)
  return td_lr

criterion = nn.MSELoss()
# criterion = RMSELoss()
model = _2TUnet()#(Bottleneck, [3, 4, 6, 3])
optimiser = optim.Adam(model.parameters(),lr=0.1)



def berhu_loss(y_true, y_pred, c=0.2):
    
    # Calculate the absolute error between the true and predicted values
    abs_diff = torch.abs(y_true - y_pred)
    
    # Calculate the maximum absolute error
    max_abs_diff = torch.max(abs_diff)
    
    # Calculate the threshold parameter
    threshold = c * max_abs_diff
    
    # Calculate the loss value
    if threshold == 0:
        loss = abs_diff.mean()
    else:
        # Calculate the Huber loss value
        huber_loss = torch.where(abs_diff < threshold, 
                                 0.5 * abs_diff ** 2, 
                                 c * (abs_diff - (0.5 * threshold)))
        
        # Calculate the absolute value of the Huber loss
        loss = torch.mean(torch.abs(huber_loss))
    
    return loss
def sobel(preds,img):
  # tensor = torch.rand([1,1, 183, 275])
  weights_x = torch.tensor([[1., 0., -1.],
                          [2., 0., -2.],
                          [1., 0., -1.]])
  weights_y = torch.tensor([[1., 2., 1.],
                          [0., 0., 0.],
                          [-1., -2., -1.]])

  if img.shape[0] == 4:
    weights_x = weights_x.view(1, 1, 3, 3).repeat(1, 4, 1, 1)
    weights_y = weights_y.view(1, 1, 3, 3).repeat(1, 4, 1, 1)
  elif img.shape[0]==1:
    weights_x = weights_x.view(1, 1, 3, 3).repeat(1, 1, 1, 1)
    weights_y = weights_y.view(1, 1, 3, 3).repeat(1, 1, 1, 1)
  weights_x= weights_x.cuda()
  weights_y= weights_y.cuda()

  pred_output_x = F.conv2d(preds, weights_x)
  img_output_x = F.conv2d(img, weights_x)

  pred_output_y = F.conv2d(preds, weights_y)
  img_output_y = F.conv2d(img, weights_y)

  output = torch.mean(torch.abs(pred_output_x-img_output_x) + torch.abs(pred_output_y-img_output_y)).data.cpu().numpy()

  # output = torch.abs(output_x) + torch.abs(output_y)
  return output

def ssim_loss(img, preds):
  def ssim_enh(img):
    weights = torch.tensor([[0.,-1.,0.],
                            [-1.,5.,-1.],
                            [0.,-1.,0.]])
    weights = weights.view(1, 1, 3, 3).repeat(1, 4, 1, 1)
    weights = weights.cuda()
    out = F.conv2d(img, weights,stride=1, padding=1)
    return out
  print(np.squeeze(ssim_enh(img).detach().cpu().numpy()).shape)
  print(np.squeeze(preds.detach().cpu().numpy()).shape)
  final_out = (1 - np.mean(ssim(np.squeeze(ssim_enh(img).detach().cpu().numpy()),np.squeeze(preds.detach().cpu().numpy()),win_size = 3)))
  return final_out


def depth_loss_function(predicted_depth, ground_truth_depth):
    
    # Point-wise depth
    ##l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)
    ber_loss = berhu_loss(ground_truth_depth,predicted_depth)
    sobel_loss = sobel(predicted_depth, ground_truth_depth)
    
    # ssim_ = ssim_loss(ground_truth_depth,predicted_depth)
    # print(ber_loss)
    # print(sobel_loss)
    # print(ssim_)

    # Weights
    w1 = 1.0
    w2 = 1.0
    w3 = 1.0

    return  (w1 * ber_loss) + (w2 * sobel_loss) #+ (w3 * ssim_)

#from torchsummary import summary
!pip install albumentations
!echo "$(pip freeze | grep albumentations) is successfully installed"
#summary(model,[(3,512,256),(3,512,256)])
#import torchvision.transforms as T
#from albumentations.pytorch import ToTensorV2
import albumentations as T
transform = T.Compose([T.RandomSizedCrop(min_max_height=(50,250),height=256, width=512,p=0.9)],
                      additional_targets={'image0': 'image', 'image1': 'image', 'image2': 'image', 'image3': 'image'}
                      )

transform4LeftRight = T.Compose([#T.RandomBrightnessContrast(p=0.5)#,
                                #  T.RandomGamma(p=0.5),
                                #  T.Blur(p=0.5),
                                #  T.Emboss(alpha=(0.2,0.7),strength=(0.2,0.7),always_apply=False, p=0.5)
                                #  T.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=1)#,
                                 #T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                                 ],
                                additional_targets={'image0': 'image', 'image1': 'image'}
                                )
#A_img = transform(TrainImgLoader)

def train(epoch, trainloader, optimizer, criterion,testloader):
    running_loss = 0.0
    count = 0 
    eps = 1e-3

    #f, axarr = plt.subplots(2,2,figsize=(10, 10))
    #g, axart = plt.subplots(2,2,figsize=(10, 10))
    
    for i, data in enumerate(trainloader, 0):#(tqdm(trainloader), 0):
        count += 1
        batch_size = 4

        #data_augmenatation
        for j in range(0,batch_size):
            Aug_img = transform(image = np.transpose(data[0][j].numpy(),(1,2,0)),
                    image1= np.transpose(data[1][j].numpy(),(1,2,0)),
                    image2= np.stack((data[2][j].numpy(),)*3, axis=-1),
                    image3= np.stack((data[3][j].numpy(),)*3, axis=-1)
                    )
            B_img = transform4LeftRight(image=Aug_img['image'],
                                        image1=Aug_img['image1'],
                                        )
            Aug_img['image'] = B_img['image']
            Aug_img['image1'] = B_img['image1']
            Aug_img['image2'] = 255 * (Aug_img['image2'] - Aug_img['image2'].min())/(Aug_img['image2'].max() - Aug_img['image2'].min()+eps)
            Aug_img['image3'] = 255 * (Aug_img['image3'] - Aug_img['image3'].min())/(Aug_img['image3'].max() - Aug_img['image3'].min()+eps)
            #print('Aug_img[image2] ' + str(Aug_img['image2'].shape))
            #print('Aug_img[image3] ' + str(Aug_img['image3'].shape))


            data[0][j] = torch.from_numpy(np.transpose(Aug_img['image'],(2,0,1)))
            data[1][j] = torch.from_numpy(np.transpose(Aug_img['image1'],(2,0,1)))
            #depth_gt = torch.from_numpy((np.transpose(Aug_img['image2'],(2,0,1)))[0])
            #depth_cl = torch.from_numpy((np.transpose(Aug_img['image3'],(2,0,1)))[0])
            data[2][j] = torch.from_numpy((np.transpose(Aug_img['image2'],(2,0,1)))[0])
            data[3][j] = torch.from_numpy((np.transpose(Aug_img['image3'],(2,0,1)))[0])

            #axart[0,0].imshow(Aug_img['image'])
            #axart[0,1].imshow(Aug_img['image1'])
            #axart[1,0].imshow(Aug_img['image2'])#.squeeze(2))
            #axart[1,1].imshow(Aug_img['image3'])#.squeeze(2))
            
        # get the inputs
        l = data[0].float()
        r = data[1].float()
        depth = data[2].float()
        #print("depth shape = " +str(depth.shape))
        depth_clue = data[3].unsqueeze(1).float()
        #print("depth_clue = " +str(depth_clue.shape))
        
        #axarr[0,0].imshow(np.transpose(data[0][0].numpy(),(1,2,0)))
        #axarr[0,1].imshow(np.transpose(data[1][0].numpy(),(1,2,0)))
        #axarr[1,0].imshow(data[2][0].numpy()/255)#.squeeze(0)/255)
        #axarr[1,1].imshow(data[3][0].numpy()/255)#.squeeze(0)/255)

        
        
        if torch.cuda.is_available():
            l, r, depth, depth_clue  = l.cuda(), r.cuda(), depth.cuda(), depth_clue.cuda()

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        predicted_depth =torch.squeeze(model(l,r,depth_clue))
        predicted_depth = 255*((predicted_depth-torch.min(predicted_depth))/(torch.max(predicted_depth)-torch.min(predicted_depth)))

        # adding one dimension to image for new_loss calculation ~ Mohit
        # predicted_depth = predicted_depth.unsqueeze(0)
        # depth = depth.unsqueeze(0)
        # print("depth shape = " + str(depth.shape))
        # print("predicted_depth shape = " + str(predicted_depth.shape))        
        loss = depth_loss_function(predicted_depth, depth)
        # print(loss)
        loss.backward()
        optimizer.step()
        # print statistics
        # if count%25 == 1:
        #   plotter(epoch,TestImgLoader)
        running_loss += loss.item()

    print('epoch %d training loss: %.3f' %
            (epoch + 1, running_loss / (len(trainloader))))
    return (epoch, (running_loss / (len(trainloader))))

def val(epoch, trainloader, criterion):
    running_loss = 0.0
    count = 0
    for i, data in enumerate(trainloader,0):#(tqdm(trainloader), 0):
        count += 1
        # get the inputs
        l = data[0].float()
        r = data[1].float()
        depth = data[2].float()
        depth_clue = data[3].float()
        if torch.cuda.is_available():
            l, r, depth, depth_clue  = l.cuda(), r.cuda(), depth.cuda(), depth_clue.cuda()

        # forward + backward + optimize
        predicted_depth =torch.squeeze(model(l,r,depth_clue.unsqueeze(1)))
        predicted_depth = 255*((predicted_depth-torch.min(predicted_depth))/(torch.max(predicted_depth)-torch.min(predicted_depth)))
        predicted_depth = predicted_depth.unsqueeze(0)
        depth = depth.unsqueeze(0)
        loss = depth_loss_function(predicted_depth, depth)
        # print statistics
        running_loss += loss.item()
    print('epoch %d validation loss: %.3f' %
            (epoch + 1, running_loss / (len(trainloader))))
    return (epoch ,running_loss / (len(trainloader)))
    
def plotter(epoch, trainloader):
    running_loss = 0.0
    count = 0
    for i, data in enumerate(trainloader,0):#(tqdm(trainloader), 0):
        # get the inputs
        count += 1
        l = data[0].float()
        r = data[1].float()
        depth = data[2].float()
        depth_clue = data[3].float()
        if torch.cuda.is_available():
            l, r, depth_clue  = l.cuda(), r.cuda(), depth_clue.cuda() 
        # forward + backward + optimize
        sh = l.shape
        dummy = torch.from_numpy(np.ones(sh)).float().cuda()
        predicted_depth_stereo =torch.squeeze(model(l,r,depth_clue.unsqueeze(1))).detach().cpu().numpy()
        #predicted_depth_midas =torch.squeeze(model(l,l)).detach().cpu().numpy()
        image = torch.squeeze(l).cpu().numpy()
        depth = torch.squeeze(depth).numpy()
        image = np.transpose(image,(1,2,0))
        image = image.astype(np.uint8)
        
        predicted_depth_stereo = (predicted_depth_stereo-predicted_depth_stereo.min())/(predicted_depth_stereo.max()-predicted_depth_stereo.min())
        #predicted_depth_midas = (predicted_depth_midas-predicted_depth_midas.min())/(predicted_depth_midas.max()-predicted_depth_midas.min())
        depth = (depth-depth.min())/(depth.max()-depth.min())
        our_ssim = np.round(ssim(predicted_depth_stereo,depth,data_range = 1),2)
        #midas_ssim = np.round(ssim(predicted_depth_midas,depth,data_range = 1),2)
        if (epoch%100 == 0):
          fig,ax = plt.subplots(1,3,figsize= (10,10))
          ax[0].imshow(image)
          ax[0].set_title('image')
          # ax[0].imshow(image[:,:,3:],alpha = 0.5)
          ax[1].imshow(predicted_depth_stereo)
          ax[1].set_title('our stereo depth')
          ax[1].set_xlabel('ssim :' + str(our_ssim))
          #ax[2].imshow(predicted_depth_midas)
          #ax[2].set_title('stereoWithBothL')
          #ax[2].set_xlabel('ssim :' + str(midas_ssim))
          ax[2].imshow(depth)
          ax[2].set_title('GT')
          plt.show()
          ssim_lis.append((epoch,our_ssim))
        #if count == 2:
        #  break
    
import matplotlib.pyplot as plt

from skimage.metrics import structural_similarity as ssim

ssim_lis = []
# file  = open('ssim.txt','w+')
model = model.cuda()
# model.load_state_dict(torch.load( '/content/MyDrive/MyDrive/Model_files/Stereo_dataset_with_central_view/13000_epoch_model11_exp_lr_eff.pt'))
# optimiser.load_state_dict(torch.load('/content/drive/MyDrive/DDP/HDR/2TUNet_optimiser(20+35)'+'.pt'))
for epoch in tqdm(range(0,40000)):
  lr = time_decay(epoch)
  for param_group in optimiser.param_groups:
    param_group['lr'] = lr
  runningLoss = 0.0
  print(' training epoch: ',epoch +1)
  train_loss = train(epoch,TrainImgLoader,optimiser,criterion,TestImgLoader)
  val_loss = val(epoch,TestImgLoader,depth_loss_function)
  plotter(epoch,TestImgLoader)
  if epoch%50 ==0:
    # print(train_loss)
    # print(val_loss)
    with open("/content/MyDrive/MyDrive/Model_files/Stereo_dataset_with_central_view/model11_exp_lr_eff.txt", 'a') as f:
      f.write(str(train_loss)+ ", "+str(val_loss)+'\n')
  if epoch %100==0:
    print(ssim_lis)
  if (epoch % 500 ==0):
    torch.save(model.state_dict(), '/content/MyDrive/MyDrive/Model_files/Stereo_dataset_with_central_view/' + str(epoch) + '_epoch_model11_exp_lr_eff.pt')
  #torch.save(optimiser.state_dict(), '/content/MyDrive/MyDrive/2TUNet/IC3D2021_2T_UNET/model_saved/2TUNet_optimizer_with_depth_hints(20).pt')


import matplotlib.pyplot as plt
with open("/content/MyDrive/MyDrive/Model_files/Stereo_dataset_with_central_view/model10_eff.txt",'r') as f:
  lis = f.readlines()

import pandas as pd
df = pd.DataFrame(lis)
df = df[0].str.replace('(','')
df = df.str.replace(')','')
df = df.str.replace('\n','')
df = df.str.split(',', expand=True)
df = df.apply(pd.to_numeric)
df = df.drop_duplicates(subset=0)
df = df.drop(2, axis=1)
df.index = df[0]
df = df.drop(0, axis=1)
df.columns = ['train_loss','val_loss']
df.plot(figsize=(20,10))

plt.ylabel("Error")
plt.xlabel("Epochs")

ssim_lis = [(0, 0.37), (100, 0.44), (200, 0.63), (300, 0.62), (400, 0.56), (500, 0.68), (600, 0.5), (700, 0.58), (800, 0.44), (900, 0.74), (1000, 0.69), (1100, 0.53), (1200, 0.67), (1300, 0.73), (1400, 0.74), (1500, 0.61), (1600, 0.62), (1700, 0.65), (1800, 0.59), (1900, 0.61), (2000, 0.61), (2100, 0.61), (2200, 0.59), (2300, 0.64), (2400, 0.65), (2500, 0.6), (2600, 0.54), (2700, 0.62), (2800, 0.52), (2900, 0.67), (3000, 0.58), (3100, 0.62), (3200, 0.6), (3300, 0.65), (3400, 0.63), (3500, 0.67), (3600, 0.67), (3700, 0.7), (3800, 0.69), (3900, 0.63), (4000, 0.69), (4100, 0.7), (4200, 0.71), (4300, 0.7), (4400, 0.69), (4500, 0.69), (4600, 0.7), (4700, 0.71), (4800, 0.71), (4900, 0.71), (5000, 0.71), (5100, 0.71), (5200, 0.7), (5300, 0.72), (5400, 0.72), (5500, 0.71), (5600, 0.72), (5700, 0.72), (5800, 0.68), (5900, 0.7), (6000, 0.72), (6100, 0.7), (6200, 0.72), (6300, 0.71), (6400, 0.69), (6500, 0.7), (6600, 0.73), (6700, 0.74), (6800, 0.72), (6900, 0.74), (7000, 0.73), (7100, 0.73), (7200, 0.73), (7300, 0.73), (7400, 0.74), (7500, 0.74), (7600, 0.75), (7700, 0.75), (7800, 0.74), (7900, 0.74), (8000, 0.73), (8100, 0.73), (8200, 0.73), (8300, 0.74), (8400, 0.74), (8500, 0.73), (8600, 0.74), (8700, 0.75), (8800, 0.73), (8900, 0.73), (9000, 0.73), (9100, 0.73), (9200, 0.73), (9300, 0.74), (9400, 0.74), (9500, 0.75), (9600, 0.76), (9700, 0.75), (9800, 0.74), (9900, 0.73), (10000, 0.72), (10100, 0.74), (10200, 0.73), (10300, 0.74), (10400, 0.75), (10500, 0.74), (10600, 0.75), (10700, 0.75), (10800, 0.72), (10900, 0.74), (11000, 0.75), (11100, 0.72), (11200, 0.75), (11300, 0.73), (11400, 0.75), (11500, 0.7), (11600, 0.78), (11700, 0.73), (11800, 0.77), (11900, 0.77), (12000, 0.77), (12100, 0.76), (12200, 0.76), (12300, 0.75), (12400, 0.72), (12500, 0.73), (12600, 0.75), (12700, 0.72), (12800, 0.76), (12900, 0.73), (13000, 0.73), (13100, 0.72), (13200, 0.74), (13300, 0.75), (13400, 0.74), (13500, 0.75), (13600, 0.75), (13700, 0.74), (13800, 0.74), (13900, 0.74), (14000, 0.74), (14100, 0.73), (14200, 0.72), (14300, 0.73), (14400, 0.74), (14500, 0.74), (14600, 0.75), (14700, 0.74), (14800, 0.74), (14900, 0.74), (15000, 0.76), (15100, 0.74), (15200, 0.76), (15300, 0.74), (15400, 0.76), (15500, 0.75), (15600, 0.76), (15700, 0.74), (15600, 0.72), (15700, 0.72), (15800, 0.73), (15900, 0.73), (16000, 0.7), (16100, 0.71), (16200, 0.7), (16300, 0.69), (16400, 0.73), (16500, 0.73), (16600, 0.7), (16700, 0.75), (16800, 0.72), (16900, 0.61), (17000, 0.74), (17100, 0.77), (17200, 0.78), (17300, 0.45), (17400, 0.76), (17500, 0.74), (17600, 0.65), (17700, 0.76), (17800, 0.72), (17900, 0.74), (18000, 0.6), (18100, 0.73), (18200, 0.66), (18300, 0.6), (18400, 0.57), (18500, 0.51), (18600, 0.5), (18700, 0.57), (18800, 0.58), (18900, 0.71), (19000, 0.64), (19100, 0.7), (19200, 0.68), (19300, 0.6), (19400, 0.73), (19500, 0.63), (19600, 0.76), (19700, 0.76), (19800, 0.6), (19900, 0.7), (20000, 0.64), (20100, 0.55), (20200, 0.57), (20300, 0.62), (20400, 0.75), (20500, 0.6)]
plt.figure(figsize=(20,10))
plt.plot(*zip(*ssim_lis), label="SSIM")
plt.title("SSIM score")
plt.ylabel("SSIM score")
plt.xlabel("Epochs")
plt.show()

